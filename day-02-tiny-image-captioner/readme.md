Day 2 â€” Tiny image captioner  
- Summary: upload image â†’ get a caption from a vision+LLM model.  
- Contract: input = image; output = 1â€“2 captions; success = appropriate caption.  
- Stack: Python FastAPI, Streamlit, LLAVA image vision model  
- Time: 2â€“3 hours.  


# Tiny Image Captioner (streaming)

This project is a small demo that uploads an image and streams back a caption generated by a vision+LLM pipeline.

Summary
- Upload an image from the web UI (Streamlit frontend).
- The backend (FastAPI) forwards the image to a model service and returns a streaming response (SSE-like JSON chunks).
- The frontend displays the uploaded image (thumbnail) and incrementally renders the caption as it arrives.

Quick checklist
- [x] Backend: `backend/api.py` â€” FastAPI endpoint `/upload_stream` that accepts form field `file` (image) and returns a text/event-stream of JSON payloads.
- [x] Frontend: `frontend/front_streamlit.py` â€” Streamlit chat-like UI, small upload button, single-line prompt, and streaming display.

Requirements
- Python 3.8+ (the repo was tested with Python 3.13)
- Backend dependencies: see `backend/requirements.txt` (FastAPI, httpx, pillow, etc.)
- Frontend dependencies: see `frontend/requirements.txt` (streamlit, requests, pillow)

Running locally (PowerShell)

1) Start the backend (FastAPI)

```powershell
cd .\backend
pip install -r .\requirements.txt
# Run with Uvicorn on port 8000 (adjust host/port as needed)
uvicorn api:app --host 0.0.0.0 --port 8000
```

The backend exposes:
- GET / -> basic health ping
- POST /upload_stream -> accepts a multipart form with field name `file` (image). The endpoint will stream a text/event-stream of JSON chunks with fields similar to:

```json
{"model":"llava","response":"Ex","done":false}
```

The frontend expects streamed JSON chunks with a `response` string and a `done` boolean. The backend may also send error objects like `{"error":"..."}`.

2) Start the frontend (Streamlit)

```powershell
cd ..\frontend
pip install -r .\requirements.txt
python -m streamlit run .\front_streamlit.py --server.port 8501 --server.headless true
```

Open http://localhost:8501 in your browser.

Frontend notes
- The default streaming endpoint is `http://localhost:8000/upload_stream` and can be changed in the Streamlit sidebar.
- Upload button is compact (ðŸ“· Upload). Images are displayed as small thumbnails in the chat UI.
- Prompt is a single-line input. Click Send to upload and start streaming.
- Optional: provide an Authorization header in the sidebar if your backend requires it.
- There's a "Simulate streaming locally" checkbox that will fake streaming output without calling the backend â€” useful for offline UI testing.

API contract (frontend â†” backend)
- Request: multipart/form-data POST to `/upload_stream` with field `file` containing the image. Optional form field `prompt` is sent by the frontend (as JSON or form data) depending on backend integration.
- Response: text/event-stream where each event contains a JSON object. The frontend parses the `response` field and appends it to the assistant message until `done` is true.

Troubleshooting
- If the Streamlit CLI is not found, use `python -m streamlit run ...` instead.
- If you see duplicated text in the UI, ensure you are running the latest `front_streamlit.py` â€” the app now updates a single chat state rather than writing both to a placeholder and session state.
- If uploads fail, check CORS settings and that the backend is reachable at the configured endpoint/port.

Next improvements (ideas)
- Add a cancel button to abort an in-flight streaming request.
- Per-message placeholders to reduce full-chat re-renders during token streaming.
- Add image preview modal when clicking thumbnails.

Contact / Credits
- Small demo prepared for a learning workshop. Adapt or extend for production with auth, input validation, and rate limiting.



